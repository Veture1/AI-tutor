# ğŸ”§ DPO + Quantization: Fine-tuning and Compressing LLMs with LoRA

This project demonstrates a full pipeline of fine-tuning and compressing a large language model (LLM) using Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and model quantization (GPTQ), with Hugging Face's `transformers`, `peft`, and `trl` libraries. The final model is optimized for efficient inference and compatible with customized evaluation APIs.

æœ¬é¡¹ç›®å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–æµç¨‹ï¼Œæ¶µç›–ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰ã€åå¥½å­¦ä¹ ï¼ˆDPOï¼‰ã€ä»¥åŠæ¨¡å‹é‡åŒ–ï¼ˆGPTQï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨äº† Hugging Face ç¤¾åŒºçš„å·¥å…·åº“å¦‚ `transformers`ã€`peft` å’Œ `trl`ï¼Œæœ€ç»ˆç”Ÿæˆçš„æ¨¡å‹å¯ä»¥åœ¨èµ„æºå—é™ç¯å¢ƒä¸­é«˜æ•ˆæ¨ç†ï¼Œå¹¶ç¬¦åˆé¡¹ç›®å®šåˆ¶åŒ–è¯„ä¼°æ¥å£çš„è¦æ±‚ã€‚

---

## ğŸ“Œ Project Structure é¡¹ç›®ç»“æ„

```
llm-dpo-lora-project/
â”œâ”€â”€ dpo_train_with_lora.py         # DPOè®­ç»ƒè„šæœ¬
â”œâ”€â”€ quantization_colab.ipynb       # GPTQé‡åŒ–å®éªŒ
â”œâ”€â”€ saved_model/                   # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ sft_test/                      # LoRAé€‚é…å™¨æƒé‡ï¼ˆç”¨äºåŠ è½½ï¼‰
â”œâ”€â”€ README.md
```

---

## ğŸš€ Pipeline Overview é¡¹ç›®æµç¨‹æ¦‚è§ˆ

### ğŸ”¹ Milestone 1 â€“ Supervised Fine-tuning (SFT)

- Dataset: [`Tachi67/sft_dataset`](https://huggingface.co/datasets/Tachi67/sft_dataset)
- Base model: `bigscience/bloom-1b7`
- Fine-tuned using `LoRA` adapter to reduce training cost.
- Although this part was led by a teammate, I also participated in training improvements due to their absence during key phases.

ä½¿ç”¨ SFT æ•°æ®é›†å¯¹ Bloom æ¨¡å‹è¿›è¡Œåˆæ­¥å¾®è°ƒï¼Œå¼•å…¥äº† LoRA ä»¥é™ä½å‚æ•°è§„æ¨¡ã€‚è¯¥é˜¶æ®µè™½ç”±é˜Ÿå‹ä¸»å¯¼ï¼Œä½†ç”±äºå…¶é˜¶æ®µæ€§ä¸åœ¨æœ¬åœ°ï¼Œæˆ‘ååŠ©è¿›è¡Œäº†è®­ç»ƒæµç¨‹çš„è°ƒæ•´å’Œè°ƒä¼˜ã€‚

---

### ğŸ”¹ Milestone 2 â€“ Direct Preference Optimization (DPO)

- Dataset: [`Tachi67/mnlp_dpo_data_7k`](https://huggingface.co/datasets/Tachi67/mnlp_dpo_data_7k)
- Framework: `trl.DPOTrainer`
- Strategy: Load SFT-trained LoRA adapter and continue optimizing using DPO loss.
- Result: Successfully adapted the SFT model to better reflect preference-style learning.

åœ¨ LoRA é€‚é…å™¨åŸºç¡€ä¸Šç»§ç»­å¾®è°ƒï¼Œä½¿ç”¨ DPO æ–¹æ³•æ›¿ä»£ä¼ ç»Ÿ RLHFï¼Œä»¥åå¥½æ•°æ®ä¼˜åŒ–æ¨¡å‹å¯¹ç”Ÿæˆè´¨é‡çš„åˆ¤æ–­èƒ½åŠ›ã€‚

---

### ğŸ”¹ Milestone 3 â€“ Quantization (GPTQ)

- Methods tested: SmoothQuant, AWQ, AQLM, Quanto, GPTQ
- Final solution: [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)  
- Issues with other methods:
  - LoRA compatibility
  - Saving format incompatible with evaluation API
- GPTQ used mixed int8/fp16 quantization, guided with DPO dataset
- Performed on [Google Colab notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)

æœ€ç»ˆæˆ‘ä»¬é€‰ç”¨ GPTQ æ–¹æ³•å®Œæˆæ¨¡å‹å‹ç¼©ï¼Œå› å…¶å¯¹ LoRA æ”¯æŒè¾ƒå¥½ä¸”å…¼å®¹ Hugging Face APIï¼Œå¯é¡ºåˆ©ä¿å­˜å¹¶ç”¨äºä¸‹æ¸¸æ¨ç†ä»»åŠ¡ã€‚

---

## ğŸ§  Key Insights é¡¹ç›®æ”¶è·

- Learned how to integrate LoRA with different fine-tuning paradigms.
- Understood the structural differences between SFT and DPO training logic.
- Gained hands-on experience debugging quantization toolkits in constrained environments.
- Developed the ability to choose between engineering trade-offs in model deployment.

---

## ğŸ¤– Environment è¿è¡Œç¯å¢ƒ

```bash
transformers==4.36+
peft==0.7+
trl==0.7+
accelerate
auto-gptq
torch==2.0+
```

---

## ğŸ‘©â€ğŸ’» Author ä½œè€…

**Zimu Zhao**  
MSc in Digital Humanities, EPFL  
Focus: AI model tuning, resource-efficient training, and cross-modal interaction design.  
ä¸ªäººå…³æ³¨æ–¹å‘ï¼šAI æ¨¡å‹å¾®è°ƒã€ä½èµ„æºæ¡ä»¶ä¸‹çš„æ•ˆç‡ä¼˜åŒ–ï¼Œä»¥åŠäººæœºäº¤äº’è®¾è®¡åœ¨è¯­è¨€ä»»åŠ¡ä¸­çš„å®ç°ã€‚

---

## ğŸ“¬ Contact è”ç³»æ–¹å¼

æ¬¢è¿ä¸æˆ‘è”ç³»è®¨è®º LoRAã€DPO æˆ–æ¨¡å‹å‹ç¼©ç­‰ç›¸å…³è¯é¢˜ï¼  
Feel free to reach out if you'd like to discuss anything related to LoRA, DPO, or quantization strategies.
